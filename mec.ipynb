{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE64Ci6RVRpOBgQGRT0YgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhivyadharsin/dhivyadharsin/blob/main/mec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yCMHbnXmoh6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4492a2da-d5c1-46cf-d8ea-b1acfd9813f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "# Save utils.py inside Google Colab session\n",
        "%%writefile utils.py\n",
        "\n",
        "import numpy as np\n",
        "import pickle  # for saving/loading data\n",
        "\n",
        "# Utility functions\n",
        "def dBm(dBm): return 10**((dBm-30)/10)\n",
        "def dB(dB): return 10**(dB/10)\n",
        "def to_dB(x): return 10*np.log10(x)\n",
        "def MHz(Mhz): return Mhz*10**6\n",
        "def GHz(GHz): return GHz*10**9\n",
        "def msec(msec): return msec*10**(-3)\n",
        "def Mbits(Mbits): return Mbits*10**6\n",
        "def mW(mW): return mW*10**(-3)\n",
        "\n",
        "# Save and load functions\n",
        "def save_data(obj, filepath):\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_data(filepath):\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "print(\"✅ utils.py saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile system_paras.py\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from utils import MHz, dBm, dB  # ✅ Import functions from utils.py\n",
        "\n",
        "# System Parameters\n",
        "num_slots = 15000\n",
        "slot_len = 10e-3\n",
        "total_time = int(num_slots * slot_len)\n",
        "time_max = 1 + np.int64(total_time / slot_len)\n",
        "\n",
        "num_users = 8\n",
        "bw_uplink = MHz(0.2)  # ✅ No more NameError!\n",
        "limit_channel_UAV = 2\n",
        "limit_channel_BS = 2\n",
        "bw_total_uav = bw_uplink * limit_channel_UAV\n",
        "bw_total_mbs = bw_uplink * limit_channel_BS\n",
        "\n",
        "# Noise & Path Loss\n",
        "noise_density = dBm(-174)\n",
        "sigma_sq_uav = bw_total_uav * noise_density\n",
        "sigma_sq_mbs = bw_total_mbs * noise_density\n",
        "g0 = dB(-50)\n",
        "gamma = 2.7601\n",
        "\n",
        "# Computing Parameters\n",
        "cycles_per_bit = 737.5\n",
        "kappa = 0.1e-27\n",
        "Vlyapunov = 1e9\n",
        "\n",
        "# Neural Network Parameters\n",
        "learning_rate = 1e-3\n",
        "training_interval = 20\n",
        "epochs = 1\n",
        "Memory = 1024\n",
        "batch_size = 256\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "print(\"✅ system_paras.py saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqes38tLTnv_",
        "outputId": "cce2b9d4-0dbe-4c6a-d210-ff6244fcf743"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing system_paras.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import utils\n",
        "import system_paras\n",
        "\n",
        "importlib.reload(utils)\n",
        "importlib.reload(system_paras)\n",
        "\n",
        "print(\"✅ Modules imported successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RQtLWDRToKe",
        "outputId": "47f2ca7d-a922-4f06-da93-a445246676d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ utils.py saved successfully!\n",
            "✅ system_paras.py saved successfully!\n",
            "✅ utils.py saved successfully!\n",
            "✅ system_paras.py saved successfully!\n",
            "✅ Modules imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Install necessary libraries in Google Colab (if not installed)\n",
        "!pip install tensorflow numpy matplotlib pandas scikit-learn -q\n",
        "\n",
        "# ✅ Import Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "\n",
        "print(f\"✅ TensorFlow Version: {tf.__version__}\")\n",
        "print(\"✅ Required libraries imported.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLuiXD_jTr92",
        "outputId": "25ce35a8-92c3-4e84-f34e-1c48cc23b6e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TensorFlow Version: 2.18.0\n",
            "✅ Required libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qWy5XTHjTsI2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MECEnvironment:\n",
        "    \"\"\"Simulates the MEC environment with dynamic task arrivals, bandwidth, and resource constraints.\"\"\"\n",
        "\n",
        "    def __init__(self, num_users=8, max_queue=1000, bandwidth=10, energy_limit=500):\n",
        "        self.num_users = num_users\n",
        "        self.max_queue = max_queue  # Maximum queue length\n",
        "        self.bandwidth = bandwidth  # Available bandwidth\n",
        "        self.energy_limit = energy_limit  # Energy threshold\n",
        "        self.queue_lengths = np.zeros(num_users)  # Task queues per user\n",
        "        self.energy_usage = np.zeros(num_users)  # Energy consumption per user\n",
        "\n",
        "    def step(self, actions):\n",
        "        \"\"\"Update environment based on DRL agent's actions (offloading decisions).\"\"\"\n",
        "        actions = np.array(actions).flatten()  # Ensure actions is a NumPy array (array of size num_users)\n",
        "\n",
        "        new_tasks = np.random.randint(1, 10, size=self.num_users)  # New task arrivals\n",
        "        self.queue_lengths += new_tasks  # Add new tasks to the queue\n",
        "\n",
        "        # Apply offloading decisions (1 = offload, 0 = local execution)\n",
        "        for i in range(self.num_users):\n",
        "            action = actions[i]  # Directly use actions[i] (correctly indexed now)\n",
        "            if action == 1:  # Offload\n",
        "                self.queue_lengths[i] -= min(self.queue_lengths[i], self.bandwidth // self.num_users)\n",
        "                self.energy_usage[i] += np.random.uniform(5, 15)  # Energy cost of offloading\n",
        "            else:  # Local execution\n",
        "                self.queue_lengths[i] -= min(self.queue_lengths[i], 5)  # Local CPU processing\n",
        "                self.energy_usage[i] += np.random.uniform(2, 10)  # Energy cost\n",
        "\n",
        "        self.queue_lengths = np.maximum(self.queue_lengths, 0)  # Prevent negative queues\n",
        "\n",
        "        # Compute reward function based on multiple metrics\n",
        "        latency = np.mean(self.queue_lengths)\n",
        "        energy_cost = np.mean(self.energy_usage)\n",
        "        task_success_rate = np.sum(self.queue_lengths < self.max_queue) / self.num_users\n",
        "        utilization = np.mean(self.bandwidth / (self.queue_lengths + 1))\n",
        "\n",
        "        reward = -(0.3 * latency + 0.2 * energy_cost - 0.3 * task_success_rate + 0.2 * utilization)\n",
        "        return self.queue_lengths, reward\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
        "        self.queue_lengths = np.zeros(self.num_users)  # Reset the task queue lengths\n",
        "        self.energy_usage = np.zeros(self.num_users)  # Reset the energy usage\n",
        "        return self.queue_lengths\n"
      ],
      "metadata": {
        "id": "vIaXt7dETyT2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DRLAgent:\n",
        "    \"\"\"Deep Reinforcement Learning Agent for optimizing MEC resource management.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, num_users, learning_rate=0.001, gamma=0.95, epsilon=1.0, epsilon_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.num_users = num_users  # Store num_users to manage actions for each user\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.epsilon_decay = epsilon_decay  # Decay rate for exploration\n",
        "        self.epsilon_min = 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "        self.memory = []\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # Build the DRL model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Builds a Deep Q-Network (DQN) for decision-making.\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),  # Input layer for state\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(self.action_size, activation='linear')  # Output layer for action size\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state):\n",
        "        \"\"\"Store experience in replay memory.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state))\n",
        "        if len(self.memory) > 1000:  # Limit memory size\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Selects an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # Random actions for all users (0 or 1 for offload or local execution)\n",
        "            return np.random.randint(0, 2, size=self.num_users)  # Correct action size matching num_users\n",
        "\n",
        "        q_values = self.model.predict(np.array(state).reshape(1, -1), verbose=0)\n",
        "        return np.argmax(q_values, axis=1)  # Returns actions for all users (num_users actions)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains the DRL model using replay memory.\"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return  # Not enough experiences to train\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        next_states = np.array(next_states)\n",
        "        rewards = np.array(rewards).reshape(-1, 1)\n",
        "        actions = np.array(actions)\n",
        "\n",
        "        # Compute target Q-values\n",
        "        target_qs = self.model.predict(states, verbose=0)\n",
        "        next_qs = self.model.predict(next_states, verbose=0)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            action_index = actions[i]  # Action for each user\n",
        "            target_q = rewards[i] + self.gamma * np.max(next_qs[i])\n",
        "            target_qs[i, action_index] = target_q\n",
        "\n",
        "        # Train the model with the updated Q-values\n",
        "        self.model.fit(states, target_qs, epochs=1, verbose=0)\n",
        "\n",
        "        # Decay exploration rate for epsilon-greedy policy\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "ACrVLl28TyGv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOAgent:\n",
        "    \"\"\"Proximal Policy Optimization (PPO) Agent for optimizing MEC resource management.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=0.2, epsilon_decay=0.995, lam=0.95):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.epsilon = epsilon  # Clipping factor for PPO\n",
        "        self.epsilon_decay = epsilon_decay  # Exploration decay\n",
        "        self.epsilon_min = 0.01  # Minimum exploration factor\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lam = lam  # Lambda for GAE (Generalized Advantage Estimation)\n",
        "\n",
        "        # Experience buffer for storing states, actions, rewards, etc.\n",
        "        self.memory = []\n",
        "\n",
        "        # Actor-Critic model: the actor predicts the action probabilities, and the critic evaluates the action.\n",
        "        self.actor = self.build_actor_model()\n",
        "        self.critic = self.build_critic_model()\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
        "\n",
        "    def build_actor_model(self):\n",
        "        \"\"\"Builds the actor (policy) network.\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(self.action_size, activation='softmax')  # Action probabilities\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def build_critic_model(self):\n",
        "        \"\"\"Builds the critic (value) network.\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Input(shape=(self.state_size,)),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(1, activation='linear')  # Value prediction for the given state\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, old_prob, done):\n",
        "        \"\"\"Store experience in replay memory.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, old_prob, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Select an action using the actor (policy) network.\"\"\"\n",
        "        state = np.array(state).reshape(1, -1)\n",
        "        prob = self.actor.predict(state)\n",
        "        action = np.random.choice(self.action_size, p=prob[0])  # Sample an action based on probabilities\n",
        "        return action, prob[0][action]\n",
        "\n",
        "    def compute_advantages(self, rewards, values, next_values, dones):\n",
        "        \"\"\"Compute the advantages using GAE (Generalized Advantage Estimation).\"\"\"\n",
        "        advantages = np.zeros_like(rewards)\n",
        "        last_advantage = 0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if dones[t]:\n",
        "                delta = rewards[t] - values[t]\n",
        "            else:\n",
        "                delta = rewards[t] + self.gamma * next_values[t] - values[t]\n",
        "            advantages[t] = last_advantage = delta + self.gamma * self.lam * last_advantage\n",
        "        return advantages\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the PPO model using the collected experiences.\"\"\"\n",
        "        if len(self.memory) < 32:\n",
        "            return  # Not enough experiences to train\n",
        "\n",
        "        states, actions, rewards, next_states, old_probs, dones = zip(*self.memory)\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        next_states = np.array(next_states)\n",
        "        old_probs = np.array(old_probs)\n",
        "        dones = np.array(dones)\n",
        "\n",
        "        values = self.critic.predict(states)  # Get current value estimates\n",
        "        next_values = self.critic.predict(next_states)  # Get next state value estimates\n",
        "        advantages = self.compute_advantages(rewards, values, next_values, dones)\n",
        "\n",
        "        # Update the actor and critic models\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Actor loss\n",
        "            prob = self.actor(states)\n",
        "            prob_actions = tf.gather(prob, actions, axis=1, batch_dims=1)\n",
        "            ratio = prob_actions / old_probs\n",
        "            surrogate_loss = ratio * advantages\n",
        "            clipped_loss = tf.clip_by_value(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantages\n",
        "            actor_loss = -tf.reduce_mean(tf.minimum(surrogate_loss, clipped_loss))\n",
        "\n",
        "            # Critic loss\n",
        "            value_loss = tf.reduce_mean(tf.square(rewards - self.critic(states)))\n",
        "\n",
        "        # Compute the gradients and apply them\n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        critic_grads = tape.gradient(value_loss, self.critic.trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "        self.optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "\n",
        "        # Decay exploration rate\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # Clear the memory buffer after training\n",
        "        self.memory.clear()\n"
      ],
      "metadata": {
        "id": "mCM3gpqCTx-W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy torch  # Add this cell if using PyTorch\n",
        "import numpy as np\n",
        "import torch  # If using PyTorch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqrfdHanT8ti",
        "outputId": "1bfa95d3-ae61-4f2c-9ba3-8be8275d67e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5insgFjT8lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ds03lUkRTxyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGSWjzAIUFmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kXuRXSiUF_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}